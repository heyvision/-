### 1. SVM
1. 公式推导
2. 核函数 

### 2. PCA
### 3. BOOSTING（分类）
1. Adaboost  
    AdaBoost方法是一种迭代算法，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率。每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低；相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。虽然AdaBoost方法对于噪声数据和异常数据很敏感。但相对于大多数其它学习算法而言，却又不会很容易出现过拟合现象。  
    在具体实现上，最初令每个样本的权重都相等，对于第k次迭代操作，我们就根据这些权重来选取样本点，进而训练分类器Ck。然后就根据这个分类器，来提高被它分错的的样本的权重，并降低被正确分类的样本权重。然后，权重更新过的样本集被用于训练下一个分类器Ck。整个训练过程如此迭代地进行下去。最后，把所有的分类器Ck线性组合起来。
2. xgboost
3. GBDT  

### 4. Bagging
### 5. 决策树（分类、回归）